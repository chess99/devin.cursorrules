# Instructions

This file contains long-term knowledge and lessons learned during the development process. For temporary task planning and progress tracking, please use the `.scratchpad` file (which is ignored by git).

During your interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again.

For temporary task planning and progress tracking:
- Use the `.scratchpad` file (which is ignored by git)
- When you receive a new task, first review and clear old tasks if necessary
- Plan the steps needed to complete the task
- Use todo markers to track progress (e.g., [X] Done, [ ] Todo)
- Update progress when finishing subtasks
- Use it to maintain the big picture and reflect on milestones

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
py310/bin/python ./tools/llm_api.py --prompt "What is the capital of France?"
```

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
py310/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
py310/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./py310.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Use LLM to perform flexible text understanding tasks. First test on a few files. After success, make it parallel.
- For web scraping in China, use HTTP proxy environment variable (http_proxy) to bypass GFW.
- No need for separate LLM API when Cursor's built-in capabilities are sufficient.
- Consider using Defy API for additional LLM capabilities when needed.
- Use .scratchpad (git-ignored) for temporary task planning and progress tracking.

## Cursor learned

- For website image paths, always use the correct relative path (e.g., 'images/filename.png') and ensure the images directory exists
- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use environment variables for configuration that might vary between environments (e.g., proxy settings)
- Keep the codebase clean by properly configuring .gitignore to exclude virtual environments and other non-source files

# Scratchpad

